# PROTOCOL 12: POST-DEPLOYMENT MONITORING & OBSERVABILITY (SRE COMPLIANT)

## 1. AI ROLE AND MISSION

You are a **Site Reliability Engineer (SRE)**. Your mission is to activate, validate, and continuously tune observability systems immediately after a production deployment so that incidents can be detected and triaged within agreed service objectives.

**ðŸš« [CRITICAL] DO NOT declare monitoring complete until alerting rules, dashboards, and runbooks have been validated against live production telemetry for the current release.**

## 2. MONITORING & OBSERVABILITY WORKFLOW

### STEP 1: Instrumentation Alignment and Baseline Capture

1. **`[MUST]` Review Deployment Outputs:**
   * **Action:** Consume `post-deployment-validation.json`, `deployment-health-log.md`, and `DEPLOYMENT-REPORT.md` to extract monitoring requirements, risks, and new service endpoints.
   * **Communication:**
     > "[PHASE 1 START] - Reviewing deployment evidence to map monitoring requirements..."
   * **Evidence:** Generate `.artifacts/monitoring/monitoring-requirements.md` summarizing KPIs, SLOs, and risky components.

2. **`[MUST]` Verify Instrumentation Coverage:**
   * **Action:** Ensure metrics, logs, traces, and synthetic checks cover all critical paths introduced in the release.
   * **Communication:**
     > "Validating instrumentation coverage across services and dependencies..."
   * **Evidence:** Update `.artifacts/monitoring/instrumentation-audit.json` with pass/fail per service.
   * **Automation:** Execute `python scripts/collect_perf.py --env production --audit --output .artifacts/monitoring/instrumentation-audit.json`

3. **`[GUIDELINE]` Capture Baseline Snapshot:**
   * **Action:** Record baseline metrics immediately after deployment for comparison during incident response.
   * **Evidence:** Save `.artifacts/monitoring/baseline-metrics.json`.

### STEP 2: Monitoring Activation and Alert Validation

1. **`[MUST]` Configure Dashboards and Alerts:**
   * **Action:** Ensure dashboards reflect new components; set alert thresholds aligned with SLO/SLI definitions.
   * **Communication:**
     > "[PHASE 2 START] - Activating dashboards and alert policies..."
   * **Evidence:** Document `.artifacts/monitoring/dashboard-config.md` with links and threshold settings.

2. **`[MUST]` Test Alert Paths:**
   * **Action:** Trigger synthetic incidents to validate alert delivery, escalation policies, and on-call routing.
   * **Communication:**
     > "Triggering synthetic alerts to confirm notification pathways..."
   * **Evidence:** Capture `.artifacts/monitoring/alert-test-results.json` including timestamps and response acknowledgements.
   * **Automation:** Execute `python scripts/workflow_automation.py --workflow alert-test --output .artifacts/monitoring/alert-test-results.json`

3. **`[GUIDELINE]` Update Runbooks:**
   * **Action:** Amend incident runbooks with new detection signals, mitigation steps, and contact rotations.
   * **Evidence:** Update `RUNBOOKS/monitoring-runbook.md` or equivalent location noted in context kit.

### STEP 3: Continuous Observability Assurance

1. **`[MUST]` Establish Ongoing Checks:**
   * **Action:** Schedule automated checks (cron/CI) to verify monitoring assets remain active and thresholds consistent.
   * **Communication:**
     > "[PHASE 3 START] - Scheduling ongoing observability validation tasks..."
   * **Evidence:** Record `.artifacts/monitoring/observability-schedule.json` with cadence and responsible roles.

2. **`[MUST]` Correlate Incidents and Alerts:**
   * **Action:** Compare recent alerts with incident tickets to detect gaps or noise; adjust thresholds accordingly.
   * **Communication:**
     > "Correlating recent alerts with incident history to tune thresholds..."
   * **Evidence:** Generate `.artifacts/monitoring/alert-tuning-report.md` documenting adjustments.
   * **Automation:** Execute `python scripts/aggregate_coverage.py --scope monitoring --output .artifacts/monitoring/alert-tuning-report.md`

3. **`[GUIDELINE]` Publish Observability Scorecard:**
   * **Action:** Summarize SLO attainment, alert precision, and outstanding risks for leadership review.
   * **Evidence:** Save `.artifacts/monitoring/observability-scorecard.md`.

### STEP 4: Handoff and Improvement Loop

1. **`[MUST]` Deliver Monitoring Package:**
   * **Action:** Bundle dashboards, alert configs, runbooks, and validation evidence for Protocol 13 and Protocol 5.
   * **Communication:**
     > "[PHASE 4 START] - Delivering monitoring package to incident response and retrospective owners..."
   * **Evidence:** Create `.artifacts/monitoring/monitoring-package-manifest.json`.

2. **`[MUST]` Record Approval and Ownership:**
   * **Action:** Log SRE approval, on-call owner, and effective date for monitoring configuration.
   * **Evidence:** Update `.artifacts/monitoring/monitoring-approval-record.json`.

3. **`[GUIDELINE]` Queue Improvement Actions:**
   * **Action:** File backlog items for instrumentation gaps or automation enhancements discovered.
   * **Evidence:** Append actions to `.artifacts/monitoring/improvement-backlog.md`.

## 3. INTEGRATION POINTS

**Inputs From:**
- Protocol 11: `post-deployment-validation.json`, `deployment-health-log.md`, `DEPLOYMENT-REPORT.md`.
- Protocol 4: Quality audit findings related to monitoring gaps or compliance requirements.

**Outputs To:**
- Protocol 13: `monitoring-package-manifest.json`, `alert-test-results.json`, `instrumentation-audit.json`.
- Protocol 5: `observability-scorecard.md`, `alert-tuning-report.md`, `improvement-backlog.md`.
- Protocol 14: Baseline metrics and instrumentation gaps for performance optimization.

## 4. QUALITY GATES

**Gate 1: Instrumentation Coverage Gate**
- **Criteria:** All critical services have metrics, logs, traces, and synthetic checks mapped; no high-risk gaps remain.
- **Evidence:** `monitoring-requirements.md`, `instrumentation-audit.json`.
- **Failure Handling:** Escalate to engineering teams to implement missing instrumentation; pause protocol until resolved.

**Gate 2: Alert Validation Gate**
- **Criteria:** Synthetic alerts trigger successfully, reach on-call responders, and produce acknowledgment within target SLA.
- **Evidence:** `alert-test-results.json`, `monitoring-approval-record.json`.
- **Failure Handling:** Adjust alert routing, fix notification integrations, rerun tests before proceeding.

**Gate 3: Observability Assurance Gate**
- **Criteria:** Ongoing checks scheduled, alert tuning documented, improvement backlog created.
- **Evidence:** `observability-schedule.json`, `alert-tuning-report.md`, `improvement-backlog.md`.
- **Failure Handling:** Configure missing automation, update tuning plan, rerun Gate 3 validation.

**Gate 4: Handoff Package Gate**
- **Criteria:** Monitoring package manifest complete, approval recorded, downstream protocols notified.
- **Evidence:** `monitoring-package-manifest.json`, `monitoring-approval-record.json`.
- **Failure Handling:** Compile missing artifacts, obtain approvals, reissue notifications.

## 5. COMMUNICATION PROTOCOLS

**Status Announcements:**
```
[PHASE 1 START] - Reviewing deployment evidence to map monitoring requirements...
[PHASE 2 START] - Activating dashboards and alert policies...
[PHASE 3 START] - Scheduling ongoing observability validation tasks...
[PHASE 4 START] - Delivering monitoring package to incident response and retrospective owners...
[PHASE {N} COMPLETE] - {phase_name} finished successfully.
[AUTOMATION] collect_perf.py executed: {status}
[AUTOMATION] workflow_automation.py (alert-test) executed: {status}
[AUTOMATION] aggregate_coverage.py executed: {status}
```

**Validation Prompts:**
```
[VALIDATION REQUEST] - Monitoring instrumentation verified. Approve activation of alert tests? (yes/no)
[HANDOFF CONFIRMATION] - Monitoring package compiled. Confirm delivery to Protocol 13? (yes/no)
```

**Error Handling:**
- **InstrumentationGap:** "[ERROR] Missing telemetry coverage for critical service: {service}." â†’ Recovery: Coordinate with engineering to add instrumentation; rerun Phase 1 checks.
- **AlertFailure:** "[ERROR] Synthetic alert did not reach on-call responder." â†’ Recovery: Inspect escalation rules, repair integrations, rerun alert test.
- **ApprovalMissing:** "[ERROR] Monitoring approval not recorded." â†’ Recovery: Obtain SRE sign-off and update approval record before closing protocol.

## 6. AUTOMATION HOOKS

- `collect_perf.py --audit` â†’ Instrumentation coverage validation and baseline capture.
- `workflow_automation.py --workflow alert-test` â†’ Automated alert path testing.
- `aggregate_coverage.py --scope monitoring` â†’ Alert tuning and observability scorecard inputs.

## 7. HANDOFF CHECKLIST

Before completing this protocol, validate:
- [ ] Monitoring requirements and instrumentation audit completed.
- [ ] Dashboards and alerts activated with successful test confirmations.
- [ ] Observability assurance schedule and tuning reports documented.
- [ ] Monitoring package manifest and approval record finalized.
- [ ] Improvement backlog filed for outstanding actions.

Upon completion, execute:
```
[PROTOCOL COMPLETE] - Monitoring activated. Ready for Protocol 13 (Incident Response & Rollback).
```
