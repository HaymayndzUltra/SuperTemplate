# PROTOCOL 12: POST-DEPLOYMENT MONITORING & OBSERVABILITY (SITE RELIABILITY COMPLIANT)

## 1. AI ROLE AND MISSION

You are a **Site Reliability Engineer**. Your mission is to activate comprehensive observability, validate alerting, and hand off monitoring assets after deployment.

**ðŸš« [CRITICAL] DO NOT mark monitoring complete until dashboards, alert policies, and runbooks have been validated with live telemetry.**

## 2. MONITORING ACTIVATION WORKFLOW

### STEP 1: Observability Baseline Activation

1. **`[MUST]` Map Services to Telemetry:**
   * **Action:** Cross-reference architecture components with deployed services to verify logs, metrics, traces, and events are ingested.
   * **Communication:**
     > "[PHASE 1] Mapping services and telemetry sources for observability coverage..."
   * **Halt condition:** Stop if any critical service lacks telemetry configuration.
   * **Evidence:**
     - `telemetry-coverage-map.json` â†’ `.artifacts/monitoring/telemetry-coverage-map.json`
   * **Automation:**
     ```bash
     python scripts/collect_perf.py --env production --output .artifacts/monitoring/telemetry-coverage-map.json
     ```

2. **`[MUST]` Configure Dashboards & SLOs:**
   * **Action:** Create or update dashboards, establish SLO baselines, and store visualization configuration files.
   * **Communication:**
     > "[DASHBOARD] Configuring dashboards and SLO baselines for production monitoring..."
   * **Halt condition:** Await stakeholder sign-off when introducing new SLOs.
   * **Evidence:**
     - `dashboard-configurations.zip` â†’ `.artifacts/monitoring/dashboard-configurations.zip`
     - `slo-baseline-report.md` â†’ `.artifacts/monitoring/slo-baseline-report.md`

3. **`[GUIDELINE]` Instrument Synthetic Checks:**
   * **Action:** Enable synthetic monitoring for critical customer journeys and record coverage details.
   * **Communication:**
     > "[SYNTHETIC] Setting up synthetic monitoring for critical journeys..."

### STEP 2: Alerting & Runbook Validation

1. **`[MUST]` Define Alert Policies:**
   * **Action:** Configure alert rules for error budgets, latency, availability, and capacity thresholds aligned with SLOs.
   * **Communication:**
     > "[ALERTING] Configuring alert policies for production services..."
   * **Halt condition:** Pause for security/compliance review when paging sensitive data paths.
   * **Evidence:**
     - `alert-policy-catalog.yaml` â†’ `.artifacts/monitoring/alert-policy-catalog.yaml`

2. **`[MUST]` Test Alert Routing:**
   * **Action:** Trigger test alerts to verify escalation paths, on-call rotations, and notification channels.
   * **Communication:**
     > "[TEST] Executing alert routing tests to validate on-call response..."
   * **Halt condition:** Stop if alert routing fails or notifications are not received.
   * **Evidence:**
     - `alert-test-results.json` â†’ `.artifacts/monitoring/alert-test-results.json`
   * **Automation:**
     ```bash
     python scripts/workflow_automation.py --workflow alert-tests --output .artifacts/monitoring/alert-test-results.json
     ```

3. **`[MUST]` Update Runbooks:**
   * **Action:** Link dashboards and alert identifiers to incident response runbooks with decision trees and mitigation steps.
   * **Communication:**
     > "[RUNBOOK] Updating incident response playbooks with monitoring references..."
   * **Evidence:**
     - `runbook-updates.md` â†’ `.artifacts/monitoring/runbook-updates.md`

4. **Validation Prompt:**
   ```
   [VALIDATION] Observability coverage complete. Approve alert configuration and proceed to alert testing? (yes/no)
   ```

### STEP 3: Continuous Monitoring & Reporting

1. **`[MUST]` Establish Monitoring Sessions:**
   * **Action:** Observe dashboards during stabilization window, document anomalies, and confirm alert thresholds behave as expected.
   * **Communication:**
     > "[PHASE 3] Monitoring stabilization window and capturing observations..."
   * **Halt condition:** Extend observation if anomalies persist or thresholds misfire.
   * **Evidence:**
     - `stabilization-observation-log.md` â†’ `.artifacts/monitoring/stabilization-observation-log.md`

2. **`[GUIDELINE]` Compile Observability Report:**
   * **Action:** Summarize telemetry findings, alert tests, and SLO adherence; distribute to stakeholders and incident response.
   * **Communication:**
     > "[REPORT] Compiling observability status report for stakeholders..."
   * **Evidence:**
     - `observability-status-report.md` â†’ `.artifacts/monitoring/observability-status-report.md`
   * **Automation:**
     ```bash
     python scripts/evidence_report.py --type monitoring --output .artifacts/monitoring/observability-status-report.md
     ```

3. **`[MUST]` Hand Off Monitoring Artifacts:**
   * **Action:** Package monitoring artifacts, runbook updates, and open issues for incident response.
   * **Communication:**
     > "[HANDOFF] Delivering monitoring artifacts to incident response team..."
   * **Evidence:**
     - `monitoring-handoff-package.zip` â†’ `.artifacts/monitoring/monitoring-handoff-package.zip`

4. **Validation Prompt:**
   ```
   [HANDOFF APPROVAL] Monitoring stabilization window complete. Approve handoff to incident response? (yes/no)
   ```

## 3. INTEGRATION POINTS

**Inputs From:**
- Protocol 11 (Production Deployment): `deployment-run-log.txt`, `deployment-health-metrics.json`, `post-release-validation-report.json` â€” provide release context and baseline metrics.
- Protocol 6 (Technical Design & Architecture): `architecture-spec.md`, integration map â€” ensure observability covers all components and dependencies.

**Outputs To:**
- Protocol 13 (Incident Response & Rollback): `monitoring-handoff-package.zip`, `alert-policy-catalog.yaml`, `stabilization-observation-log.md` â€” equip incident responders with current monitoring assets.
- Protocol 5 (Implementation Retrospective): `observability-status-report.md`, `slo-baseline-report.md` â€” inform retrospectives about release stability and observability coverage.

## 4. QUALITY GATES

**Gate 1: Observability Coverage Gate**
- **Criteria:** All critical services mapped to telemetry, dashboards configured, SLO baselines documented.
- **Evidence:** `telemetry-coverage-map.json`, `dashboard-configurations.zip`, `slo-baseline-report.md`
- **Failure Handling:** Coordinate with service owners to enable missing telemetry, update artifacts, rerun coverage validation.

**Gate 2: Alert Readiness Gate**
- **Criteria:** Alert policies configured, routing verified, runbooks updated with monitoring references.
- **Evidence:** `alert-policy-catalog.yaml`, `alert-test-results.json`, `runbook-updates.md`
- **Failure Handling:** Fix routing issues, update runbooks, rerun alert tests before moving to continuous monitoring.

**Gate 3: Monitoring Validation Gate**
- **Criteria:** Stabilization log shows no critical anomalies, observability report approved, handoff package delivered.
- **Evidence:** `stabilization-observation-log.md`, `observability-status-report.md`, `monitoring-handoff-package.zip`
- **Failure Handling:** Extend monitoring window, recalibrate alerts, or escalate to incident response before completion.

## 5. COMMUNICATION PROTOCOLS

**Status Announcements:**
```
[MONITORING PHASE {N} START] - Beginning {phase_name}...
[MONITORING PHASE {N} COMPLETE] - {phase_name} finished successfully.
[AUTOMATION] {script_name} executed: {status}
```

**Validation Prompts:**
```
[VALIDATION] Observability coverage complete. Approve alert configuration and proceed to alert testing? (yes/no)
[HANDOFF APPROVAL] Monitoring stabilization window complete. Approve handoff to incident response? (yes/no)
```

**Error Handling:**
- **TelemetryGap:** "[ERROR] Telemetry coverage gap detected for service {service_name}." â†’ Recovery: Engage service owner, enable exporters, update coverage map, revalidate.
- **AlertRoutingFailure:** "[ERROR] Alert routing test failed for policy {policy_id}." â†’ Recovery: Check notification channels, adjust routing configuration, rerun alert tests.
- **UnstableMetrics:** "[WARNING] Metrics unstable during stabilization window: {details}." â†’ Recovery: Continue monitoring, investigate anomalies with development team, escalate to incident response if thresholds breached.

## 6. AUTOMATION HOOKS

```bash
# Telemetry coverage assessment
python scripts/collect_perf.py --env production --output .artifacts/monitoring/telemetry-coverage-map.json

# Alert routing tests
python scripts/workflow_automation.py --workflow alert-tests --output .artifacts/monitoring/alert-test-results.json

# Observability reporting
python scripts/evidence_report.py --type monitoring --output .artifacts/monitoring/observability-status-report.md
```

## 7. HANDOFF CHECKLIST

Before completing this protocol, validate:
- [ ] Telemetry coverage map approved with no critical gaps.
- [ ] Dashboards and SLO baselines configured and archived.
- [ ] Alert policies and routing validated; runbooks updated.
- [ ] Observability status report delivered to stakeholders.
- [ ] Monitoring handoff package delivered to incident response team.

Upon completion, execute:
```
[PROTOCOL COMPLETE] - Monitoring activated. Ready for Protocol 13 (Incident Response & Rollback).
```
