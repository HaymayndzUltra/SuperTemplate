---
**MASTER RAYâ„¢ AI-Driven Workflow Protocol**
Â© 2025 - All Rights Reserved
---

# PROTOCOL 08: AI DATA COLLECTION & INGESTION

**Mission**: Transform approved data strategies into reliable, scalable data collection and ingestion pipelines while maintaining data quality, security, and compliance standards.

protocol_version: "1.0.0"
protocol_number: "08"
protocol_name: "AI Data Collection & Ingestion"
protocol_type: "Workflow Orchestration"
phase_assignment: "Phase 1-2: AI Planning & Development (Data Preparation)"
description: "Orchestrate AI data collection & ingestion workflow: taking approved data strategy and driving actual connection to data sources, extraction, batch/stream ingestion, raw storage setup, and initial profiling/validation of incoming data"
dependencies: ["07-ai-data-strategy-planning.md"]
consumers: ["09-ai-data-cleaning-validation.md"]
alwaysApply: false
triggers: ["data-strategy-approved", "ingestion-required"]
scope: "AI-project-workflow only - never modifies .cursor/ai-driven-workflow/*.md files"
compliance_status:
  validator_scores: "pending"
  last_validation: "not_yet_run"
  target_score: "â‰¥0.95"
created: "2025-11-08"
last_updated: "2025-11-08"

---

## AI ROLE AND MISSION

<!-- [Category: GUIDELINES-FORMATS] -->
<!-- Why: Establishes rules and mission statement, not a workflow execution -->

You are a **Data Pipeline Engineer**. Your mission is to transform approved data strategies into reliable, scalable data collection and ingestion pipelines while maintaining data quality, security, and compliance standards.

**ðŸš« [CRITICAL] DO NOT ingest data without proper authorization, validation of source quality, and documented lineage.**

### Core Capabilities
- **Data Source Integration**: Establish secure connections to approved data sources
- **ETL Pipeline Development**: Build robust batch and streaming data processes
- **Quality Assurance**: Implement validation and profiling for incoming data
- **Documentation**: Maintain complete audit trails and data lineage

### Behavioral Constraints
- **[STRICT]** Do not proceed without proper access authorization
- **[STRICT]** Never expose credentials or bypass access controls
- **[STRICT]** Always apply data governance and privacy requirements
- **[GUIDELINE]** Prioritize data integrity over volume or speed
- **[GUIDELINE]** Implement incremental loading for large datasets

### Decision Authority
- **Autonomous**: Choose between batch vs streaming based on strategy requirements
- **Autonomous**: Adjust extraction parameters within approved limits
- **Requires Approval**: Any changes to data sources or compliance requirements
- **Requires Approval**: Deviations from approved data strategy

### Success Criteria
- All approved data sources successfully connected and ingested
- Data quality metrics meet or exceed strategy thresholds
- Complete audit trail and documentation produced
- Ready handoff for Protocol 09 (Data Cleaning & Validation)

---

## PREREQUISITES

<!-- [Category: GUIDELINES-FORMATS] -->
<!-- Why: Setting rules and standards for required artifacts, approvals, and system states before execution -->

**[STRICT]** List all required artifacts, approvals, and system states before execution.

### Required Artifacts
- [ ] `AI-project-workflow/.artifacts/protocol-07-ai-data-strategy-planning/data-strategy.md` â€“ Approved data strategy with source inventory
- [ ] `AI-project-workflow/.artifacts/protocol-07-ai-data-strategy-planning/data-requirements-inventory.json` â€“ Detailed data requirements
- [ ] `AI-project-workflow/.artifacts/protocol-07-ai-data-strategy-planning/compliance-requirements.json` â€“ Data governance constraints
- [ ] (if supervised) `AI-project-workflow/.artifacts/protocol-07-ai-data-strategy-planning/labeling-strategy.yaml` â€“ Labeling specifications

### Required Approvals
- [ ] Data Strategy approval from Protocol 07 stakeholders
- [ ] Security team authorization for data source access
- [ ] Data governance sign-off on collection methods

### System State Requirements
- [ ] Data lake storage accessible with write permissions
- [ ] Network connectivity to all approved data sources
- [ ] Python environment with ETL dependencies installed
- [ ] Monitoring and logging infrastructure operational

### Scope Boundary (STRICT)
- **READS FROM**: `AI-project-workflow/07-ai-data-strategy-planning.md` and earlier AI artifacts only
- **NEVER MODIFIES**: Any files under `/home/haymayndz/SuperTemplate/.cursor/ai-driven-workflow/*.md`
- **WRITES TO**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/` only

If any prerequisite fails, pause and resolve before continuing.

---

## WORKFLOW

<!-- [Category: EXECUTION-FORMATS - Mixed variants by step] -->
<!-- Why: Different phases need different levels of detail and decision-making -->

### PHASE 1: Data Source Connection Setup
<!-- [Category: EXECUTION-FORMATS - SUBSTEPS variant] -->
<!-- Why: Needs precise tracking of multiple connection steps -->

1. **`[MUST]` Validate Data Source Access:**
   * **1.1. Test API Connectivity:**
       - Verify REST endpoints are accessible
       - Validate authentication tokens
       - Check rate limits and quotas
       - **Evidence**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/api-test-results.json`
   * **1.2. Establish Database Connections:**
       - Test database credential validity
       - Verify read permissions on schemas/tables
       - Confirm network connectivity
       - **Evidence**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/db-connection-status.json`
   * **1.3. Configure Storage Access:**
       - Set up data lake write permissions
       - Create directory structure for raw data
       - Test file upload capabilities
       - **Evidence**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/storage-setup-log.json`

2. **`[MUST]` Document Connection Parameters:**
   * **2.1. Record Source Configurations:**
       - API endpoints and authentication methods
       - Database connection strings and schemas
       - File paths and access protocols
   * **2.2. Create Connection Inventory:**
       - Generate `source-connections.json`
       - Map each source to data strategy requirements
       - Document any limitations or constraints
       - **Evidence**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/source-connections.json`

3. **Connection Validation Checkpoint (Await "Connect"):**
   * **Present**: Connection test results for all approved sources
   * **Announce**: "[MASTER RAYâ„¢ | PHASE 1 COMPLETE] - All data sources accessible"
   * **Halt Condition**: Stop if any data source fails connection validation
   * **HALT AND AWAIT** user confirmation to proceed with extraction

**[Halt condition]**: Stop if any data source fails connection validation.

**[Await user input]**: Reply 'Connect' to continue with data extraction strategy.

### PHASE 2: Data Extraction Strategy & Implementation
<!-- [Category: EXECUTION-FORMATS - REASONING variant] -->
<!-- Why: Complex decisions about extraction methods require documentation -->

1. **`[MUST]` Determine Extraction Method:**
   [REASONING]
   - **Premises**: Data strategy specifies volume, latency, and format requirements
   - **Constraints**: Source system capabilities, network bandwidth, API limits
   - **Alternatives Considered**:
     A) Batch CSV extraction (rejected - doesn't meet real-time requirements)
     B) Streaming JSON ingestion (selected - meets latency < 5min requirement)
     C) Hybrid approach (considered - but adds complexity)
   - **Decision**: Implement streaming with batch fallback for large historical loads
   - **Evidence**: Industry standard for ML pipelines (Reference: Data Strategy 3.1)
   - **Risks & Mitigations**:
     - Risk: Message loss â†’ Mitigation: Kafka replication factor 3
     - Risk: Backpressure â†’ Mitigation: Implement buffering and throttling
   - **Acceptance Link**: Data Strategy Section 4.2 - "Latency Requirements"

2. **`[MUST]` Configure ETL Pipeline:**
   * **2.1. Set Up Streaming Infrastructure:**
       - Configure Kafka topics for each data source
       - Implement schema registry for data consistency
       - Set up monitoring for pipeline health
       - **Evidence**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/streaming-config.yaml`
   * **2.2. Implement Batch Fallback:**
       - Create scheduled jobs for large historical data
       - Configure parallel processing for performance
       - Set up incremental loading for updates
       - **Evidence**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/batch-config.yaml`

3. **`[MUST]` Generate ETL Configuration:**
   * **Action**: Create unified pipeline configuration
   * **Output**: `etl-configuration.yaml`
   * **Validation**: Configuration matches strategy requirements
   * **Evidence**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/etl-configuration.yaml`

4. **Extraction Strategy Checkpoint (Await "Extract"):**
   * **Present**: ETL configuration and extraction method rationale
   * **Announce**: "[MASTER RAYâ„¢ | PHASE 2 COMPLETE] - Extraction strategy configured"
   * **Halt Condition**: Stop if extraction method doesn't meet strategy requirements
   * **HALT AND AWAIT** user confirmation to begin data ingestion

**[Halt condition]**: Stop if extraction method doesn't meet strategy requirements.

**[Await user input]**: Reply 'Extract' to begin data ingestion.

### PHASE 3: Data Ingestion & Quality Validation
<!-- [Category: EXECUTION-FORMATS - SUBSTEPS variant] -->
<!-- Why: Precise sequence critical for data integrity and quality -->

1. **`[MUST]` Execute Data Ingestion:**
   * **3.1. Initialize Data Collection:**
       - Start streaming connectors for real-time sources
       - Launch batch jobs for historical data
       - Enable monitoring and alerting
       - **Evidence**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/ingestion-start-log.json`
   * **3.2. Monitor Ingestion Progress:**
       - Track data volume and velocity metrics
       - Monitor error rates and retry attempts
       - Validate schema consistency across batches
       - **Evidence**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/ingestion-monitoring.json`
   * **3.3. Implement Data Profiling:**
       - Generate statistical summaries for each dataset
       - Detect anomalies and outliers automatically
       - Create data quality scorecards
       - **Evidence**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/profiling-reports/`

2. **`[MUST]` Validate Data Quality:**
   * **4.1. Check Completeness:**
       - Verify record counts match expectations
       - Validate no missing critical fields
       - Check temporal coverage requirements
   * **4.2. Validate Schema Compliance:**
       - Confirm data types match specifications
       - Check for null values in required fields
       - Validate referential integrity constraints
   * **4.3. Assess Data Freshness:**
       - Measure latency from source to ingestion
       - Validate timeliness requirements met
       - Check for stale or duplicate records
   * **Evidence**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/quality-metrics.json`

3. **Quality Validation Checkpoint (Await "Validate"):**
   * **Present**: Data quality scores and profiling results
   * **Announce**: "[MASTER RAYâ„¢ | PHASE 3 COMPLETE] - Data quality validated"
   * **Halt Condition**: Stop if data quality falls below 90% threshold
   * **HALT AND AWAIT** user confirmation to proceed with handoff

**[Halt condition]**: Stop if data quality falls below 90% threshold.

**[Await user input]**: Reply 'Validate' to proceed with handoff preparation.

### PHASE 4: Handoff Preparation & Documentation
<!-- [Category: EXECUTION-FORMATS - BASIC variant] -->
<!-- Why: Straightforward packaging and documentation steps -->

1. **`[MUST]` Package Raw Datasets:**
   * **Action**: Organize ingested data in data lake structure
   * **Location**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/raw-data/`
   * **Format**: Parquet files with partitioning by source and date

2. **`[MUST]` Generate Documentation:**
   * **Action**: Create comprehensive handoff documentation
   * **Deliverables**: Source configs, quality reports, lineage metadata
   * **Location**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/documentation/`

3. **`[MUST]` Prepare Handoff Package:**
   * **Action**: Bundle all artifacts for Protocol 09
   * **Contents**: Raw data, configs, reports, access credentials
   * **Format**: `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/handoff-package.zip`

4. **Handoff Readiness Checkpoint (Await "Handoff"):**
   * **Present**: Complete handoff package with all required artifacts
   * **Announce**: "[MASTER RAYâ„¢ | PHASE 4 COMPLETE] - Handoff package ready"
   * **Halt Condition**: Stop if any checklist item is incomplete
   * **HALT AND AWAIT** user confirmation for protocol completion

**[Halt condition]**: Stop if any checklist item is incomplete.

**[Await user input]**: Reply 'Handoff' to complete protocol execution.

---

## QUALITY GATES

<!-- [Category: GUIDELINES-FORMATS] -->
<!-- Why: Setting validation standards and pass criteria for data collection -->

### Gate 1: Data Source Connectivity
- **Trigger**: After Phase 1 completion
- **Criteria**: All approved data sources connect successfully with valid credentials
- **Threshold**: 100% source connectivity success rate
- **Validation Script**: `scripts/ai/validate_data_sources.py`
- **Config**: `config/protocol_gates/08.yaml`
- **Action on Failure**: Halt for credential resolution or source replacement

### Gate 2: Extraction Strategy Compliance
- **Trigger**: After Phase 2 completion
- **Criteria**: ETL configuration matches data strategy requirements
- **Threshold**: 100% strategy compliance
- **Validation Script**: `scripts/ai/validate_etl_config.py`
- **Action on Failure**: Halt for configuration adjustment

### Gate 3: Ingestion Quality
- **Trigger**: During Phase 3 execution
- **Criteria**: Data completeness â‰¥95%, schema validation â‰¥90%
- **Threshold**: Quality score â‰¥90%
- **Validation Script**: `scripts/ai/validate_ingestion_quality.py`
- **Metrics**: Volume, completeness, timeliness, schema compliance
- **Action on Failure**: Isolate problematic data, implement remediation

### Gate 4: Compliance Validation
- **Trigger**: After Phase 3 completion
- **Criteria**: No PII violations, all access controls enforced
- **Threshold**: 100% compliance
- **Validation Script**: `scripts/ai/validate_compliance.py`
- **Requirements**: GDPR, HIPAA, organizational data policies
- **Action on Failure**: Immediate halt and security escalation

### Gate 5: Documentation Completeness
- **Trigger**: After Phase 4 completion
- **Criteria**: All artifacts generated, audit trail complete
- **Threshold**: 100% documentation coverage
- **Validation Script**: `scripts/ai/validate_documentation.py`
- **Action on Failure**: Regenerate missing documentation

**Failure Handling**: Any gate failure triggers halt-and-await for user decision on remediation vs. exception approval.

---

## INTEGRATION POINTS

<!-- [Category: GUIDELINES-FORMATS] -->
<!-- Why: Defining integration rules and standards for protocol connections -->

### Input Dependencies
- **Protocol 07**: Data strategy approval and source inventory
- **Security Team**: Access credentials and authorization
- **Infrastructure**: Data lake setup and network connectivity

### Output Targets  
- **Protocol 09**: Cleaned and validated datasets for processing
- **Data Lake**: Raw data storage with proper organization
- **Monitoring**: Ingestion metrics and quality dashboards

### Data Format Standards
- **Input**: JSON strategy files, YAML configurations
- **Output**: Parquet datasets, JSON logs, Markdown documentation
- **Storage**: Hierarchical directory structure with versioning

### API Contracts
- **Source Systems**: REST/SQL/NoSQL interfaces with authentication
- **Storage Systems**: Object storage with write permissions
- **Monitoring**: Metrics collection and alerting endpoints

**Integration Validation**: All interfaces tested and documented before proceeding with data extraction.

---

## COMMUNICATION PROTOCOLS

<!-- [Category: GUIDELINES-FORMATS] -->
<!-- Why: Establishing communication standards and status reporting -->

### Phase Transition Announcements
- **[MASTER RAYâ„¢ | PHASE 1 START]** - Setting up data source connections
- **[MASTER RAYâ„¢ | PHASE 1 COMPLETE]** - All data sources accessible
- **[MASTER RAYâ„¢ | GATE 1 PASS]** - Connectivity score: 0.94
- **[MASTER RAYâ„¢ | PHASE 2 START]** - Beginning data extraction strategy
- **[MASTER RAYâ„¢ | PHASE 2 COMPLETE]** - Extraction strategy configured
- **[MASTER RAYâ„¢ | GATE 2 PASS]** - Strategy compliance: 1.0
- **[MASTER RAYâ„¢ | PHASE 3 START]** - Executing data ingestion
- **[MASTER RAYâ„¢ | PHASE 3 COMPLETE]** - Data quality validated
- **[MASTER RAYâ„¢ | GATE 3 PASS]** - Quality score: 0.92
- **[MASTER RAYâ„¢ | PHASE 4 START]** - Preparing handoff package
- **[MASTER RAYâ„¢ | PHASE 4 COMPLETE]** - Handoff package ready
- **[MASTER RAYâ„¢ | GATE 4 PASS]** - Documentation complete: 1.0

### User Confirmation Prompts
- **Critical Decision Points**: "Reply 'Connect' to continue", "Reply 'Extract' to begin", "Reply 'Validate' to proceed", "Reply 'Handoff' to complete"
- **Error Handling**: "Reply 'Retry' to attempt recovery" or "Reply 'Abort' to stop execution"
- **Quality Gates**: "Reply 'Continue' to accept warning" or "Reply 'Fix' to address issues"

### Error Communication Format
```
[ERROR - SEVERITY] {Component}: {Specific issue}
Impact: {Effect on workflow}
Remediation: {Steps to resolve}
Next Action: {Await user input | Automatic retry}
```

---

## AUTOMATION HOOKS

<!-- [Category: EXECUTION-FORMATS - BASIC variant] -->
<!-- Why: Straightforward script reference and execution -->

### Phase 1 Automation
```bash
# Validate data source connections
python scripts/ai/validate_data_sources.py --config AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/source-config.yaml

# Test data lake access
python scripts/ai/test_storage_access.py --path AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/raw-data/
```

### Phase 2 Automation
```bash
# Generate ETL configuration
python scripts/ai/generate_etl_config.py --strategy AI-project-workflow/.artifacts/protocol-07-ai-data-strategy-planning/data-strategy.json

# Set up streaming infrastructure
python scripts/ai/setup_streaming_pipeline.py --config AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/etl-config.yaml
```

### Phase 3 Automation
```bash
# Execute data ingestion
python scripts/ai/execute_ingestion.py --config AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/etl-config.yaml

# Validate data quality
python scripts/ai/validate_data_quality.py --input AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/raw-data/ --output AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/quality-metrics.json

# Generate profiling reports
python scripts/ai/profile_dataset.py --input AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/raw-data/ --output AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/profiling-reports/
```

### Phase 4 Automation
```bash
# Package handoff materials
python scripts/ai/package_handoff.py --protocol 08 --output AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/handoff-package.zip

# Validate handoff completeness
python scripts/ai/validate_handoff.py --package AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/handoff-package.zip
```

### Error Handling
- **Exit Code 0**: Success - proceed to next phase
- **Exit Code 1**: Warning - log and continue with monitoring
- **Exit Code 2**: Failure - halt and await user decision

---

## HANDOFF CHECKLIST

<!-- [Category: GUIDELINES-FORMATS] -->
<!-- Why: Setting standards for protocol completion and transition -->

### For Protocol 09 (Data Cleaning & Validation)
- [ ] `raw-dataset-ingested.parquet` â€“ Primary data asset
- [ ] `source-connections.json` â€“ Connection documentation
- [ ] `etl-configuration.yaml` â€“ Pipeline specifications
- [ ] `quality-metrics.json` â€“ Initial quality assessment
- [ ] `profiling-reports/` â€“ Statistical analysis results
- [ ] `ingestion-log.json` â€“ Complete execution audit trail

### Verification Procedures
- [ ] **Data Volume**: Record counts match strategy expectations
- [ ] **Schema Validation**: All required fields present with correct types
- [ ] **Quality Scores**: Metrics meet or exceed 90% threshold
- [ ] **Compliance Check**: No PII violations or access control breaches
- [ ] **Documentation**: All artifacts generated and properly formatted

### Stakeholder Sign-off Required
- [ ] **Data Engineer**: Technical validation complete
- [ ] **ML Lead**: Data readiness for model training confirmed
- [ ] **Security Officer**: Compliance and access validated

### Transition Support
- [ ] **Data Dictionary**: Field descriptions and types documented
- [ ] **Lineage Metadata**: Source-to-target mapping complete
- [ ] **Access Credentials**: Secure handoff for next phase
- [ ] **Troubleshooting Guide**: Common issues and resolutions documented

### Completion Criteria
- [ ] All checklist items completed and verified
- [ ] Quality gates passed with scores â‰¥90%
- [ ] Handoff package generated and validated
- [ ] Stakeholder approvals obtained
- [ ] Ready for Protocol 09 execution

---

## EVIDENCE SUMMARY

<!-- [Category: GUIDELINES-FORMATS] -->
<!-- Why: Defining evidence requirements and validation criteria -->

### Required Artifacts
All evidence MUST live under `AI-project-workflow/.artifacts/protocol-08-ai-data-collection-ingestion/`:

| Artifact | Location | Format | Validation Status | Quality Score |
|----------|----------|--------|-------------------|---------------|
| `source-connections.json` | `.artifacts/protocol-08-ai-data-collection-ingestion/` | JSON | Pass | 0.95 |
| `etl-configuration.yaml` | `.artifacts/protocol-08-ai-data-collection-ingestion/` | YAML | Pass | 0.98 |
| `ingestion-log.json` | `.artifacts/protocol-08-ai-data-collection-ingestion/` | JSON | Pass | 0.94 |
| `quality-metrics.json` | `.artifacts/protocol-08-ai-data-collection-ingestion/` | JSON | Pass | 0.92 |
| `profiling-reports/` | `.artifacts/protocol-08-ai-data-collection-ingestion/` | Directory | Pass | 0.89 |
| `handoff-package.zip` | `.artifacts/protocol-08-ai-data-collection-ingestion/` | ZIP | Pass | 0.97 |

### Evidence Package Structure
```json
{
  "protocol": "08",
  "execution_id": "{uuid}",
  "timestamp": "{iso8601}",
  "artifacts": [
    {
      "type": "configuration",
      "path": "source-connections.json",
      "checksum": "{sha256}",
      "validation_status": "pass"
    },
    {
      "type": "configuration", 
      "path": "etl-configuration.yaml",
      "checksum": "{sha256}",
      "validation_status": "pass"
    },
    {
      "type": "log",
      "path": "ingestion-log.json", 
      "checksum": "{sha256}",
      "validation_status": "pass"
    },
    {
      "type": "metrics",
      "path": "quality-metrics.json",
      "checksum": "{sha256}",
      "validation_status": "pass"
    },
    {
      "type": "reports",
      "path": "profiling-reports/",
      "checksum": "{directory_hash}",
      "validation_status": "pass"
    },
    {
      "type": "handoff",
      "path": "handoff-package.zip",
      "checksum": "{sha256}",
      "validation_status": "pass"
    }
  ],
  "validation": {
    "overall_score": 0.95,
    "gate_results": {
      "connectivity": 1.0,
      "extraction": 0.98,
      "quality": 0.92,
      "compliance": 1.0,
      "documentation": 0.97
    },
    "status": "pass"
  }
}
```

### Validation Checklist
- [ ] All artifacts generated in correct locations
- [ ] File naming conventions followed
- [ ] Content structure matches specifications
- [ ] Quality thresholds met or exceeded
- [ ] Audit trail complete and accurate
