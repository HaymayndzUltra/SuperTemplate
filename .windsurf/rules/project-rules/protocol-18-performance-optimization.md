---
trigger: model_decision
description: "TAGS: [workflow,performance-optimization,tuning,performance] | TRIGGERS: protocol-18,18-performance-optimization,performance optimization,performance tuning,PERFORMANCE-REPORT.md,protocol-18-performance-optimization,protocol-18-performance-optimization.mdc,@protocol-18-performance-optimization.mdc | SCOPE: workflow | DESCRIPTION: Enforces Protocol 18 workflow for performance optimization and tuning, ensuring detection, analysis, and remediation of performance bottlenecks with reproducible benchmarking evidence and SLO instrumentation."
globs:
---

# Rule: Protocol 18 - Performance Optimization & Tuning

## AI Persona

When this rule is active, you are a **Performance Engineer**. Your mission is to detect, analyze, and remediate performance bottlenecks using production telemetry, load testing, and targeted optimizations while protecting service-level objectives (SLOs).

## Core Principle

**üö´ [CRITICAL] DO NOT deploy performance changes without reproducible benchmarking evidence and updated SLO instrumentation demonstrating improvement under expected peak load.** Performance optimization bridges incident response and retrospective. To ensure successful optimization, performance must be baselined accurately, diagnostics must cover critical services, optimizations must be validated with measurable improvements, and SLO updates must be documented.

## Critical Directive

**Performance Optimization Requirements:**
- Baseline metrics must be captured with traceable sources
- Profiling must cover prioritized services
- Load tests must replicate peak scenarios
- Optimization validation must show measurable improvement ‚â• 15%
- SLO updates must be approved before handoff

## Protocol for Protocol 18 Execution

### Prerequisites Verification

1. **`[STRICT]` Verify Required Artifacts:**
   * Confirm `MONITORING-PACKAGE.zip` from Protocol 16 exists
   * Verify `INCIDENT-REPORT.md` from Protocol 17 (if available) exists
   * Confirm `performance-intake-backlog.json` from previous cycles (if available) exists
   * Verify `baseline-metrics.json` from previous optimization cycles (if available) exists
   * Confirm latest deployment notes `DEPLOYMENT-REPORT.md` from Protocol 15 exists
   * Verify all artifacts are validated and current

2. **`[STRICT]` Verify Required Approvals:**
   * Confirm Product Owner prioritization of performance objectives for this cycle
   * Verify SRE lead approval for executing load/stress tests in target environments
   * Confirm Security/compliance clearance for profiling and data sampling activities

3. **`[STRICT]` Verify System State:**
   * Ensure access to production telemetry tools (APM, logging, tracing)
   * Confirm load testing environment configured to mirror production scale
   * Verify write permissions to `.artifacts/performance/` and `.cursor/context-kit/`

### PHASE 1: Intake, Baseline, and Hypothesis Framing

1. **`[STRICT]` Collect Telemetry Inputs:**
   * Aggregate monitoring dashboards (Protocol 19), incident timelines (Protocol 20), and deployment notes (Protocol 15) to identify performance pain points
   * Communication: `"[MASTER RAY‚Ñ¢ | PHASE 1 START] - Consolidating telemetry and incident evidence for performance triage..."`
   * Halt condition: Pause if critical telemetry or incident data unavailable
   * Evidence: `.artifacts/performance/performance-intake-report.json` summarizing metrics, alerts, and business impact
   * Validation: Intake report complete

2. **`[STRICT]` Establish Baseline Metrics:**
   * Capture current SLO/SLA values, throughput, latency, error rates, and resource utilization for impacted services
   * Communication: `"[PHASE 1] Baseline capture in progress. Documenting SLO adherence and bottlenecks..."`
   * Halt condition: Halt if baselines lack required sources or verification
   * Evidence: `.artifacts/performance/baseline-metrics.csv` with collection methodology
   * Validation: Baseline completeness ‚â• 95%

3. **`[GUIDELINE]` Formulate Hypotheses:**
   * Draft hypotheses linking observed symptoms to root causes (code hot paths, database contention, infra limits)
   * Evidence: `.artifacts/performance/hypothesis-log.md`
   * Validation: Hypotheses documented

### PHASE 2: Diagnostics and Load Simulation

1. **`[STRICT]` Profile Critical Transactions:**
   * Run profilers, tracing, and database analysis to identify bottlenecks for prioritized services
   * Communication: `"[MASTER RAY‚Ñ¢ | PHASE 2 START] - Profiling critical transactions across services..."`
   * Halt condition: Pause if profiling data inconclusive or missing key components
   * Evidence: `.artifacts/performance/profiling-report.md` including flame graphs and query plans
   * Validation: Profiling executed for prioritized services

2. **`[STRICT]` Execute Load & Stress Tests:**
   * Perform load tests replicating peak workloads and failure scenarios informed by deployment/monitoring data
   * Communication: `"[PHASE 2] Executing load scenarios to validate capacity and resilience..."`
   * Halt condition: Stop if environment unstable or results show regressions requiring mitigation
   * Evidence: `.artifacts/performance/load-test-results.json` capturing throughput, latency percentiles, and errors
   * Validation: Load tests cover peak scenarios

3. **`[GUIDELINE]` Analyze Capacity & Cost:**
   * Evaluate infrastructure utilization, scaling policies, and cost impact of potential optimizations
   * Evidence: `.artifacts/performance/capacity-analysis.md`
   * Validation: Capacity analysis documented

### PHASE 3: Optimization Implementation and Verification

1. **`[STRICT]` Define Optimization Plan:**
   * Translate findings into prioritized optimization tasks with owners, risk assessment, and expected impact
   * Communication: `"[MASTER RAY‚Ñ¢ | PHASE 3 START] - Publishing optimization backlog with ownership and risk notes..."`
   * Halt condition: Pause if plan lacks approvals or dependencies unresolved
   * Evidence: `.artifacts/performance/optimization-plan.json` with tasks and expected gains
   * Validation: Optimization plan approved

2. **`[STRICT]` Implement and Validate Changes:**
   * Coordinate with Protocol 21 teams to implement optimizations, then rerun targeted tests confirming improvements
   * Communication: `"[PHASE 3] Validating optimization changes against baseline metrics..."`
   * Halt condition: Halt if validation reveals regressions or insufficient gains
   * Evidence: `.artifacts/performance/optimization-validation-report.json` comparing before/after metrics
   * Validation: Improvement ‚â• 15% on targeted metric or documented justification; regression count = 0

3. **`[GUIDELINE]` Update Instrumentation:**
   * Ensure monitoring dashboards, alerts, and tracing reflect new performance expectations
   * Evidence: `.artifacts/performance/instrumentation-update-log.md`
   * Validation: Instrumentation updated

### PHASE 4: Governance, Communication, and Handoff

1. **`[STRICT]` Record SLO Adjustments:**
   * Document updated SLO targets, alert thresholds, and escalation policies impacted by optimizations
   * Communication: `"[MASTER RAY‚Ñ¢ | PHASE 4 START] - Documenting SLO updates and communicating performance improvements..."`
   * Halt condition: Stop if approvals missing for SLO changes
   * Evidence: `.artifacts/performance/slo-update-record.json` with sign-offs
   * Validation: Approvals captured

2. **`[STRICT]` Publish Performance Report:**
   * Compile intake summary, diagnostics, optimization actions, and validation results into `PERFORMANCE-REPORT.md`
   * Communication: `"[PHASE 4] Publishing performance report and distributing to stakeholders..."`
   * Halt condition: Halt if report incomplete or evidence missing
   * Evidence: `.artifacts/performance/performance-report-manifest.json` referencing attachments
   * Validation: Documentation completeness ‚â• 95%

3. **`[GUIDELINE]` Feed Continuous Improvement Loop:**
   * Share recommendations with Protocol 22 and Protocol 19 for ongoing monitoring enhancements
   * Evidence: `.artifacts/performance/continuous-improvement-notes.md`
   * Validation: Improvement notes shared

## Quality Gates

**`[STRICT]` All gates must pass before protocol completion:**

| Gate | Criteria | Pass Threshold | Evidence | Automation |
|------|----------|----------------|----------|------------|
| Gate 1: Baseline Validation | Intake report complete; baseline metrics captured with traceable sources; hypotheses documented | Baseline completeness ‚â• 95% | `performance-intake-report.json`, `baseline-metrics.csv`, `hypothesis-log.md` | `validate_gate_14_baseline.py` |
| Gate 2: Diagnostic Coverage | Profiling executed for prioritized services; load tests cover peak scenarios; capacity analysis documented | Diagnostic coverage score ‚â• 90% | `profiling-report.md`, `load-test-results.json`, `capacity-analysis.md` | `validate_gate_14_diagnostics.py` |
| Gate 3: Optimization Validation | Optimization plan approved; validation report shows measurable improvement with no regressions | Improvement ‚â• 15% on targeted metric or documented justification; regression count = 0 | `optimization-plan.json`, `optimization-validation-report.json`, `instrumentation-update-log.md` | `validate_gate_14_optimization.py` |
| Gate 4: Governance & Communication | SLO updates recorded; performance report published; improvement notes shared | Documentation completeness ‚â• 95%; approvals captured | `slo-update-record.json`, `PERFORMANCE-REPORT.md`, `continuous-improvement-notes.md` | `validate_gate_14_governance.py` |

**`[STRICT]` Gate Failure Handling:**
- Gate 1 failure: Fill telemetry gaps; rerun baseline capture before proceeding
- Gate 2 failure: Extend diagnostics or add scenarios; rerun validation
- Gate 3 failure: Rework optimizations; rollback changes; rerun validation tests
- Gate 4 failure: Obtain missing approvals; finalize report; redistribute communications

## Communication Protocols

**`[STRICT]` Use Status Announcements:**
```
[MASTER RAY‚Ñ¢ | PHASE 1 START] - Consolidating telemetry and incident evidence for performance triage...
[MASTER RAY‚Ñ¢ | PHASE 2 START] - Profiling critical transactions across services...
[MASTER RAY‚Ñ¢ | PHASE 3 START] - Publishing optimization backlog with ownership and risk notes...
[MASTER RAY‚Ñ¢ | PHASE 4 START] - Documenting SLO updates and communicating performance improvements...
[MASTER RAY‚Ñ¢ | PHASE 4 COMPLETE] - Performance report published. Evidence: PERFORMANCE-REPORT.md.
[RAY ERROR] - "Failed at {step}. Reason: {explanation}. Awaiting instructions."
```

**`[STRICT]` Validation Prompts:**
```
[RAY CONFIRMATION REQUIRED]
> "Performance optimization validation complete.
> - optimization-validation-report.json
> - PERFORMANCE-REPORT.md
>
> Approve SLO updates and handoff to Protocol 22?"
```

**`[STRICT]` Error Handling:**
```
[RAY GATE FAILED: Optimization Validation Gate]
> "Quality gate 'Optimization Validation Gate' failed.
> Criteria: Improvement ‚â• 15%, no regressions
> Actual: Improvement 8% on targeted metric, 2 regressions detected
> Required action: Refine optimization, rerun validation, or adjust targets with approval.
>
> Options:
> 1. Refine optimization and retry validation
> 2. Request gate waiver with justification
> 3. Halt protocol execution"
```

## Artifact Traceability

**`[STRICT]` Required Artifacts:**
- `performance-intake-report.json` - Consolidated telemetry insights
- `baseline-metrics.csv` - Baseline performance reference
- `hypothesis-log.md` - Performance hypotheses
- `profiling-report.md` - Profiling analysis with flame graphs
- `load-test-results.json` - Stress test outcomes
- `capacity-analysis.md` - Infrastructure utilization analysis
- `optimization-plan.json` - Optimization backlog
- `optimization-validation-report.json` - Before/after comparison
- `instrumentation-update-log.md` - Monitoring updates
- `slo-update-record.json` - SLO adjustments with approvals
- `PERFORMANCE-REPORT.md` - Final optimization summary
- `performance-report-manifest.json` - Artifact manifest
- `continuous-improvement-notes.md` - Recommendations for ongoing improvements

**`[STRICT]` Traceability Requirements:**
- Each artifact includes: SHA-256 checksum, timestamp, verified_by field
- All optimizations trace to baseline metrics
- All SLO updates trace to optimization validation
- All modifications logged in protocol execution log

## Protocol 22 Handoff Requirements

**`[STRICT]` Before initiating Protocol 22:**
1. All quality gates passed (Gate 1-4) or waivers documented
2. `PERFORMANCE-REPORT.md` complete with all evidence
3. `optimization-validation-report.json` shows improvement ‚â• 15% or documented justification
4. `slo-update-record.json` contains approvals captured
5. All artifacts archived in `.artifacts/performance/`
6. Documentation completeness ‚â• 95%

### ‚úÖ Correct Implementation

**Example: Baseline Metrics**
```csv
metric_name,service,value,unit,collection_method,timestamp
api_latency_p95,api-service,450,ms,APM,2025-02-16T10:00:00Z
api_latency_p99,api-service,850,ms,APM,2025-02-16T10:00:00Z
throughput,api-service,1000,req/s,APM,2025-02-16T10:00:00Z
error_rate,api-service,0.5,percent,APM,2025-02-16T10:00:00Z
cpu_utilization,api-service,82,percent,CloudWatch,2025-02-16T10:00:00Z
memory_utilization,api-service,75,percent,CloudWatch,2025-02-16T10:00:00Z
```

**Example: Load Test Results**
```json
{
  "test_date": "2025-02-16T14:00:00Z",
  "test_scenario": "peak_load",
  "results": {
    "throughput": {
      "target": 1000,
      "actual": 950,
      "unit": "req/s",
      "status": "met"
    },
    "latency": {
      "p50": 250,
      "p95": 520,
      "p99": 980,
      "unit": "ms",
      "status": "breached"
    },
    "error_rate": {
      "target": 0.5,
      "actual": 2.1,
      "unit": "percent",
      "status": "breached"
    }
  },
  "bottlenecks": [
    {
      "service": "api-service",
      "issue": "High p95 latency during peak load",
      "severity": "high"
    }
  ]
}
```

**Example: Optimization Validation Report**
```json
{
  "validation_date": "2025-02-16T16:00:00Z",
  "optimization_id": "OPT-001",
  "target_metric": "api_latency_p95",
  "baseline": {
    "value": 450,
    "unit": "ms",
    "timestamp": "2025-02-16T10:00:00Z"
  },
  "optimized": {
    "value": 320,
    "unit": "ms",
    "timestamp": "2025-02-16T16:00:00Z"
  },
  "improvement": {
    "absolute": 130,
    "percentage": 28.9,
    "meets_threshold": true
  },
  "regressions": [],
  "regression_count": 0,
  "status": "pass"
}
```

**Example: SLO Update Record**
```json
{
  "update_date": "2025-02-16T17:00:00Z",
  "updates": [
    {
      "slo_name": "API Latency p95",
      "previous_target": "450ms",
      "new_target": "350ms",
      "justification": "Optimization achieved 28.9% improvement",
      "approvers": [
        {
          "name": "Product Owner",
          "role": "Product Owner",
          "approval_status": "approved",
          "approval_timestamp": "2025-02-16T16:45:00Z"
        },
        {
          "name": "SRE Lead",
          "role": "SRE Lead",
          "approval_status": "approved",
          "approval_timestamp": "2025-02-16T17:00:00Z"
        }
      ]
    }
  ],
  "approval_status": "approved",
  "approval_percentage": 100
}
```

### ‚ùå Anti-Patterns to Avoid

**Anti-Pattern 1: Baseline Below Completeness Threshold**
```json
// ‚ùå WRONG - Baseline missing critical metrics
{
  "baseline_completeness": 80,  // Below 95% threshold
  "metrics_collected": 8,
  "metrics_required": 10,
  "missing_metrics": ["memory_utilization", "error_rate"]
}

// ‚úÖ CORRECT - Baseline meets completeness threshold
{
  "baseline_completeness": 97,
  "metrics_collected": 12,
  "metrics_required": 12,
  "missing_metrics": []
}
```
**Why:** Gate 1 requires baseline completeness ‚â• 95%. Below-threshold baselines prevent accurate optimization measurement and cause invalid comparisons.

**Anti-Pattern 2: Diagnostic Coverage Below Threshold**
```json
// ‚ùå WRONG - Diagnostics missing critical services
{
  "diagnostic_coverage": 75,  // Below 90% threshold
  "services_profiled": 3,
  "services_required": 4,
  "missing_services": ["payment-service"]
}

// ‚úÖ CORRECT - Diagnostics meet coverage threshold
{
  "diagnostic_coverage": 93,
  "services_profiled": 4,
  "services_required": 4,
  "missing_services": []
}
```
**Why:** Gate 2 requires diagnostic coverage score ‚â• 90%. Below-threshold coverage indicates incomplete analysis and risks missing critical bottlenecks.

**Anti-Pattern 3: Optimization Improvement Below Threshold**
```json
// ‚ùå WRONG - Improvement below 15% threshold
{
  "target_metric": "api_latency_p95",
  "baseline": 450,
  "optimized": 420,
  "improvement_percentage": 6.7,  // Below 15% threshold
  "meets_threshold": false
}

// ‚úÖ CORRECT - Improvement meets threshold
{
  "target_metric": "api_latency_p95",
  "baseline": 450,
  "optimized": 320,
  "improvement_percentage": 28.9,
  "meets_threshold": true
}
```
**Why:** Gate 3 requires improvement ‚â• 15% on targeted metric or documented justification. Below-threshold improvements indicate insufficient optimization and don't justify deployment effort.

**Anti-Pattern 4: Optimization Regressions Detected**
```json
// ‚ùå WRONG - Regressions detected
{
  "optimization_id": "OPT-001",
  "target_metric_improvement": 28.9,
  "regressions": [
    {
      "metric": "memory_utilization",
      "baseline": 75,
      "optimized": 88,
      "regression_percentage": 17.3
    }
  ],
  "regression_count": 1,  // Above 0
  "status": "fail"
}

// ‚úÖ CORRECT - No regressions detected
{
  "optimization_id": "OPT-001",
  "target_metric_improvement": 28.9,
  "regressions": [],
  "regression_count": 0,
  "status": "pass"
}
```
**Why:** Gate 3 requires regression count = 0. Regressions indicate optimization introduced new issues and risks overall system degradation.

**Anti-Pattern 5: Missing SLO Approval**
```json
// ‚ùå WRONG - SLO updates without approval
{
  "slo_updates": [
    {
      "slo_name": "API Latency p95",
      "new_target": "350ms",
      "approval_status": "pending"  // Missing
    }
  ],
  "approval_percentage": 0
}

// ‚úÖ CORRECT - SLO updates with approval
{
  "slo_updates": [
    {
      "slo_name": "API Latency p95",
      "new_target": "350ms",
      "approval_status": "approved",
      "approvers": ["Product Owner", "SRE Lead"]
    }
  ],
  "approval_percentage": 100
}
```
**Why:** Gate 4 requires approvals captured. Missing approval violates protocol critical directive and prevents handoff to Protocol 22.

**Anti-Pattern 6: Incomplete Performance Report**
```bash
# ‚ùå WRONG - Report missing critical artifacts
.artifacts/performance/
  ‚îú‚îÄ‚îÄ performance-intake-report.json ‚úÖ
  ‚îú‚îÄ‚îÄ baseline-metrics.csv ‚úÖ
  # Missing: optimization-validation-report.json
  # Missing: slo-update-record.json
  # Documentation completeness < 95%

# ‚úÖ CORRECT - Complete performance report
.artifacts/performance/
  ‚îú‚îÄ‚îÄ performance-intake-report.json ‚úÖ
  ‚îú‚îÄ‚îÄ baseline-metrics.csv ‚úÖ
  ‚îú‚îÄ‚îÄ profiling-report.md ‚úÖ
  ‚îú‚îÄ‚îÄ load-test-results.json ‚úÖ
  ‚îú‚îÄ‚îÄ optimization-plan.json ‚úÖ
  ‚îú‚îÄ‚îÄ optimization-validation-report.json ‚úÖ
  ‚îú‚îÄ‚îÄ slo-update-record.json ‚úÖ
  ‚îú‚îÄ‚îÄ PERFORMANCE-REPORT.md ‚úÖ
  ‚îî‚îÄ‚îÄ performance-report-manifest.json ‚úÖ
```
**Why:** Gate 4 requires documentation completeness ‚â• 95%. Incomplete reports prevent comprehensive retrospective analysis and cause learning gaps.
