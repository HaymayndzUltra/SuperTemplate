# AI Protocol Analyzer â€” Notion Template (System Playbook / 6â€‘Step)

> ğŸ¯ Goal
> 
> 
> Deterministic, auditâ€‘ready analysis and enhancement of AI instruction protocols using a fixed **6â€‘step pipeline** with exact output headers and measurable criteria.
> 
> ğŸ‘¤ **Role**: AI Protocol Analyzer
> 
> ğŸ§­ **Mode**: **Do not skip steps.** Follow the order strictly.
> 

---

## ğŸ§± Suggested Page Properties

- **Type** (Select): Metaâ€‘instruction / System instruction / Reasoning framework / Hybrid
- **Version** (Text): e.g., `1.0.0`
- **Owner** (Person): â€”
- **Source / Link** (URL): â€”
- **Created / Updated** (Date): â€”
- **Compliance (0â€‘100)** (Number): â€”

---

## ğŸš€ Quick Use

1. Paste your **TARGET PROTOCOL** below.
2. Open each **Toggle Section** in order (Steps 1â†’6).
3. Complete the tables/checklists.
4. Deliver the **Enhanced Protocol** and **Change Log** exactly as specified.

---

## ğŸ“Œ Output Headers (exact; copy/paste)

```
CLASSIFICATION
STRUCTURAL AUDIT
AMBIGUITY ANALYSIS
COMPLETENESS VERIFICATION
ENHANCED PROTOCOL
CHANGE LOG

```

---

## ğŸ§© STEP 1 â€” PROTOCOL CLASSIFICATION (Toggle)

**Instructions:** 1â€“2 sentences.

- **Protocol type:** [metaâ€‘instruction | system instruction | reasoning framework | hybrid]
- **Primary objective:** [one sentence describing the achieved outcome]

**Example:**

> This is a system instruction protocol. Its primary objective is to generate API documentation from code comments with >90% completeness.
> 

---

## ğŸ§± STEP 2 â€” STRUCTURAL AUDIT (Toggle)

**Instructions:** Create a **numbered list** of every instruction. For each, extract:

- **Instruction ID**
- **Original text (quote)**
- **Action required** (verb phrase: "extract X", "classify Y as Z", "output format Q")
- **Input dependencies** (what must exist | "none" | **UNDEFINED**)
- **Success criteria** (how to verify | **MISSING**)

**Perâ€‘Instruction Table (duplicate rows as needed):**

| ID | Original Quote | Action Required | Input Dependencies | Success Criteria | Notes |
| --- | --- | --- | --- | --- | --- |
| 1 | "â€¦" | extract/classify/outputâ€¦ | none/UNDEFINED/â€¦ | metric or **MISSING** | â€” |

**Global Findings:**

- **Circular dependencies:** Aâ†”B pairs â†’ â€¦
- **Undefined terms:** â€¦
- **Implicit assumptions:** *The protocol assumesâ€¦*

---

## ğŸ§­ STEP 3 â€” AMBIGUITY DETECTION (Toggle)

**Apply tests to each instruction from Step 2:**

**Test 1 â€” Action Clarity**

- Can it be executed via a specific algorithm/procedure?
    - **YES** â†’ `[CLEAR]`
    - **NO** â†’ `[AMBIGUOUS]` â†’ *Unclear because â€¦*

**Test 2 â€” Output Specification**

- Output format explicitly defined?
    - **YES** â†’ list format
    - **NO** â†’ `[OUTPUT UNDEFINED]`

**Test 3 â€” Branching Logic**

- Requires conditional execution?
    - **YES** but conditions missing â†’ `[MISSING CONDITIONS]`
    - **NO** â†’ `[LINEAR]`
    - **YES** with conditions â†’ list conditions

**Perâ€‘Instruction Tag Sheet (example):**

| ID | Action Clarity | Output Spec | Branching Logic | Comment |
| --- | --- | --- | --- | --- |
| 1 | CLEAR / AMBIGUOUS | DEFINED / **OUTPUT UNDEFINED** | LINEAR / **MISSING CONDITIONS** / CONDITIONS: â€¦ | â€¦ |

---

## ğŸ§® STEP 4 â€” COMPLETENESS CHECK (Toggle)

**Verify required components:**

| Component | Present? | Evidence / Gap |
| --- | --- | --- |
| Input format specification | Y/N | Quote or "Missing: no input format specified" |
| Processing steps (sequential) | Y/N | Count of steps or "Missing: only outcome described" |
| Output format specification | Y/N | Quote or "Missing: format not defined" |
| Success/failure criteria | Y/N | Quote or "Missing: no validation criteria" |
| Edge case handling | Y/N | Quote or "Missing: no error conditions addressed" |
| Termination conditions | Y/N | Quote or "Missing: completion state unclear" |

---

## ğŸ” STEP 5 â€” ENHANCED PROTOCOL GENERATION (Toggle)

**Rewrite using the transformations below. Preserve original intent.**

**Transformation A â€” Concretize Abstract Verbs**

- "Analyze X" â†’ `1) Extract [specific elements] from X 2) Classify into [category set] 3) Output as [format]`
- "Improve Y" â†’ `1) Measure Y vs [metric with units] 2) IF < [threshold] THEN apply [technique] 3) Validate via [test]`
- "Evaluate Z" â†’ `1) Score on [scale] using [rubric] 2) Document evidence as [format] 3) Compare to [baseline]`

**Transformation B â€” Externalize Implicit Information**

Add for every instruction missing Stepâ€‘4 components:

- **Input:** exact required data
- **Process:** numbered subâ€‘steps / algorithm
- **Output:** explicit format + example/template
- **Validation:** `Success if [measurable condition]`

**Transformation C â€” Eliminate Subjective Terms**

- "high quality" â†’ `error rate < 2%`
- "comprehensive" â†’ `includes all 8 required sections`
- "thorough" â†’ `minimum 3 levels of hierarchy`
- "appropriate" â†’ `matches pattern [X]`

**Transformation D â€” Specify Conditional Logic**

```
IF [measurable condition] THEN [specific action + output]
ELSE IF [measurable condition] THEN [specific action + output]
ELSE [default action + output]

```

**Enhanced Protocol Scaffold (repeat per instruction):**

```
ID N â€” [Short Title]
Input: â€¦
Process: 1) â€¦ 2) â€¦ 3) â€¦
Output: format + path/example
Validation: Success if [metric/threshold]; else trigger [fallback].

```

---

## ğŸ“ STEP 6 â€” CHANGE DOCUMENTATION (Toggle)

**Exact table (â‰¥ 3 entries when issues exist):**

| Original Instruction (quote) | Issue Category | Enhancement Applied | Measurable Improvement |
| --- | --- | --- | --- |
| "â€¦" | Ambiguous verb / Undefined output / Missing validation / Implicit dependency | â€¦ | Before: X â†’ After: Y |

> Note: If the source truly has <3 issues, state that and avoid fabricating entries.
> 

---

## ğŸ›¡ï¸ Constraints (Callout)

> Preserve Original Intent â€” donâ€™t add scope beyond objectives.Quote Precisely â€” include exact text (+ line refs if available).Flag Uncertainty â€” multiple interpretations â†’ list all with INTERPRETATION AMBIGUITY.No Capability Inference â€” donâ€™t assume unstated abilities.Evidenceâ€‘Based â€” every issue must cite protocol text.
> 

---

## ğŸš« Antiâ€‘Patterns to Avoid (Callout)

- Rating things as "good/bad" (use criteria).
- Adding scope beyond intent.
- Hedging ("might/could/possibly"): be definitive **or** mark **UNCERTAIN**.
- Skipping sections.
- Generic advice without quoting specific text.

---

## âœ… Execution Checklist

- [ ]  Steps **1â†’6** completed in order
- [ ]  Every claim cites **specific text**
- [ ]  Enhanced version has **zero subjective terms**
- [ ]  All conditionals use **IF / THEN / ELSE**
- [ ]  Change Log has **â‰¥ 3** real entries (or explicitly justified fewer)
- [ ]  No assumptions about **unstated capabilities**

---

## ğŸ—‚ï¸ TARGET PROTOCOL (paste here)

# Stage 1: Role & Intent Explainer

**Purpose:** Define the role and clarify the intent of your meta prompt. This is the foundation of your prompt building process.

---

## ğŸ“ Role Definition

**Instruction ID 1 â€” Role Persona Definition**
- **Input:** Project brief or stakeholder notes describing the AIâ€™s intended responsibilities, domain, audience, and tone.
- **Process:**
  1) Extract domain, primary responsibilities, target audience, and tone keywords from the supplied materials.
  2) If any field is missing, prompt the user to supply it before drafting.
  3) Compose a 2â€“3 sentence description covering all four elements.
- **Output:** Markdown paragraph labeled â€œRoleâ€ containing domain, responsibilities, audience, and tone in complete sentences.
- **Validation:** Success if all four elements appear explicitly; otherwise flag â€œRole description incompleteâ€ and loop to step 2.

### Role Output Template
```
Role: [Describe domain, responsibilities, audience, tone in 2â€“3 sentences.]
```

---

## ğŸ¯ Intent Explanation

**Instruction ID 2 â€” Primary Intent Statement**
- **Input:** User-provided goal statements or success criteria for the meta prompt.
- **Process:**
  1) List all stated goals.
  2) Consolidate into a single leading objective written as an outcome with measurable scope (e.g., deliverable + metric/timeline).
  3) Confirm the statement references the intended dataset or task context.
- **Output:** Single sentence labeled â€œPrimary Intentâ€ including deliverable, scope, and context.
- **Validation:** Success if sentence answers â€œWhat output?â€, â€œFor whom/what dataset?â€, and â€œHow measured?â€; otherwise request clarification.

### Primary Intent Template
```
Primary Intent: [Outcome including deliverable, scope, context, and measurement.]
```

**Instruction ID 3 â€” Secondary Objectives Listing**
- **Input:** Additional goals gathered from stakeholders.
- **Process:**
  1) Capture each distinct supporting goal.
  2) Transform each into a bullet containing action, target, and desired impact.
  3) Limit list to top five items prioritized by user; if more provided, ask for ranking.
- **Output:** Bulleted list under â€œSecondary Objectivesâ€ where each bullet follows â€œAction â†’ Target â†’ Impactâ€ structure.
- **Validation:** Success if at least one and no more than five bullets exist, each with the three fields; otherwise adjust.

### Secondary Objectives Template
```
Secondary Objectives:
- Action: [Action] â†’ Target: [Target] â†’ Impact: [Impact]
- ...
```

---

## ğŸ”¤ User Input Section

**Instruction ID 4 â€” Custom Prompt Capture**
- **Input:** Original user prompt or query text.
- **Process:**
  1) Paste supplied prompt verbatim into a fenced block.
  2) Annotate metadata line indicating source (e.g., â€œProvided by [name/date]â€).
  3) If text exceeds 500 words, confirm truncation strategy with user.
- **Output:** Quoted block labeled â€œUser Queryâ€ containing prompt plus metadata line.
- **Validation:** Success if block matches supplied text checksum and metadata line present; otherwise re-ingest.

### User Query Template
```
User Query:
> [Verbatim prompt text]
> Metadata: Provided by [name/date]
```

---

## ğŸ“¤ Output: Intent Document

### Generated Intent Document (Ready to Copy)
```
Role: [From Role Output]
Primary Intent: [From Primary Intent Template]
Secondary Objectives:
- [Action â†’ Target â†’ Impact]
User Query:
> [Verbatim prompt text]
> Metadata: Provided by [name/date]
```

**Instruction ID 5 â€” Transfer Stageâ€‘1 Output to Stageâ€‘2**
- **Input:** Completed Stageâ€‘1 Intent Document file.
- **Process:**
  1) Copy entire Stageâ€‘1 markdown.
  2) Paste into Stageâ€‘2 input field.
  3) Run diff to ensure paste length matches source length.
- **Output:** Stageâ€‘2 input field populated with identical Stageâ€‘1 document.
- **Validation:** Success if diff shows zero changes; otherwise repeat paste.

**Transfer Verification Log**
- Source File Path: __________________
- Destination Field: __________________
- Diff Result: __________________
- Timestamp: __________________

---

# Stage 2: Reasoning Synthesizer (RS)

ğŸ“

**Purpose:** Structure the logical reasoning approach based on the intent defined in Stage 1. This stage builds the cognitive framework for your meta prompt.

---

## ğŸ“¥ Input from Previous Stage

**Instruction ID 6 â€” Stageâ€‘2 Input Confirmation**
- **Input:** Stageâ€‘1 document and Stageâ€‘2 workspace.
- **Process:**
  1) Verify Stageâ€‘2 input contains headings â€œRole,â€ â€œPrimary Intent,â€ â€œSecondary Objectives,â€ and â€œUser Query.â€
  2) If any missing, re-paste Stageâ€‘1 output.
- **Output:** Stageâ€‘2 input validated checklist with four items marked complete.
- **Validation:** Success if all four headings present; otherwise block progression.

### Stageâ€‘2 Input Checklist
- [ ] Role heading present
- [ ] Primary Intent heading present
- [ ] Secondary Objectives heading present
- [ ] User Query heading present
- Validation Timestamp: __________________

---

## ğŸ§  Reasoning Framework

**Instruction ID 7 â€” Reasoning Approach Selection**
- **Input:** Intent document and list of available reasoning approaches.
- **Process:**
  1) Evaluate intent requirements (analytical depth, data type).
  2) Map requirements to selection criteria table (e.g., Deductive for rule enforcement).
  3) Choose approach with highest match score; document justification in one sentence.
- **Output:** Entry â€œSelected Approach: [Option] â€” Rationale: [Sentence].â€
- **Validation:** Success if rationale references at least one intent attribute and option is from provided list; otherwise reconsider.

### Reasoning Approach Scorecard
| Approach | Criteria Match Notes | Score (1-5) |
| --- | --- | --- |
| Deductive Reasoning | | |
| Inductive Reasoning | | |
| Abductive Reasoning | | |
| Analogical Reasoning | | |
| Causal Reasoning | | |

Selected Approach: __________________ â€” Rationale: __________________________________

**Instruction ID 8 â€” Reasoning Steps Definition**
- **Input:** Chosen reasoning approach and intent document.
- **Process:**
  1) Decompose task into minimum three sequential steps aligned with approach.
  2) For each, define input consumed and output produced.
  3) Ensure dependencies flow without gaps.
- **Output:** Numbered list â€œStep 1 â€¦ Step Nâ€ with input/output annotations.
- **Validation:** Success if â‰¥3 steps, each naming input and output; else refine.

### Reasoning Steps Template
1. Step Name â€” Input: ________ â†’ Output: ________
2. Step Name â€” Input: ________ â†’ Output: ________
3. Step Name â€” Input: ________ â†’ Output: ________
4. ...

**Instruction ID 9 â€” Key Considerations Documentation**
- **Input:** Context notes, constraints, stakeholder requirements.
- **Process:**
  1) For Context Analysis, summarize environment factors (max 3 bullets).
  2) List assumptions each with justification.
  3) Enumerate constraints with source references.
  4) Define success criteria as measurable targets (metric + threshold).
- **Output:** Table with columns Context/Assumptions/Constraints/Success Criteria containing bullet entries.
- **Validation:** Success if each column has â‰¥1 entry and success metrics include numeric or boolean thresholds; otherwise revise.

### Key Considerations Table
| Context | Assumptions | Constraints | Success Criteria |
| --- | --- | --- | --- |
| - ... | - ... | - ... | - Metric: ___ â‰¥/â‰¤ ___ |

---

## ğŸ“¤ Output: Reasoning Structure

### Generated Reasoning Structure (Ready to Copy)
```
Selected Approach: [Option] â€” Rationale: [Sentence]

Reasoning Steps:
1. [Step details]
...

Analysis Parameters (Key Considerations):
- Context: [...]
- Assumptions: [...]
- Constraints: [...]
- Success Criteria: [...]
```

**Instruction ID 10 â€” Transfer Stageâ€‘2 Output to Stageâ€‘3**
- **Input:** Completed Stageâ€‘2 Reasoning Structure.
- **Process:**
  1) Copy entire structure.
  2) Paste into Stageâ€‘3 input.
  3) Run word-count comparison (difference â‰¤0).
- **Output:** Stageâ€‘3 input populated with Stageâ€‘2 document, plus log showing counts match.
- **Validation:** Success if counts identical; else redo.

**Instruction ID 11 â€” Stageâ€‘3 Input Confirmation**
- **Input:** Stageâ€‘2 output within Stageâ€‘3 workspace.
- **Process:**
  1) Confirm presence of â€œReasoning Approach,â€ â€œReasoning Steps,â€ and â€œAnalysis Parameters.â€
  2) Mark checklist.
- **Output:** Signed checklist stored with timestamp.
- **Validation:** Success if all three headings present; otherwise re-import.

### Stageâ€‘3 Transfer Log
- Stageâ€‘2 Word Count: _______
- Stageâ€‘3 Word Count: _______
- Match Status: [ ] Pass / [ ] Fail
- Checklist:
  - [ ] Reasoning Approach present
  - [ ] Reasoning Steps present
  - [ ] Analysis Parameters present
- Reviewer Signature: __________________
- Timestamp: __________________

---

# Stage 3: Logic Planner (LP)

**Purpose:** Map out the decision-making framework and logical pathways. This stage creates the execution blueprint for your meta prompt.

---

## ğŸ—ºï¸ Logic Mapping

**Instruction ID 12 â€” Logical Flow Construction**
- **Input:** Reasoning structure and problem requirements.
- **Process:**
  1) Identify primary decision nodes; ensure each ties to a reasoning step.
  2) Map conditions and outcomes in flow diagram or table.
  3) Verify coverage of all critical scenarios defined in intent.
- **Output:** Decision tree table listing Condition, Action, Outcome for each node.
- **Validation:** Success if every reasoning step has at least one linked node and no uncovered intent scenario; otherwise expand tree.

### Decision Tree Table
| Condition | Action | Outcome | Linked Reasoning Step |
| --- | --- | --- | --- |
| If ... | Then ... | Outcome ... | Step # |

**Instruction ID 13 â€” Conditional Logic Specification**
- **Input:** Decision tree from ID12.
- **Process:**
  1) Translate each branch into IF/THEN statements.
  2) Include ELSE path covering remaining cases.
  3) Label each statement with output artifact.
- **Output:** Ordered list of IF/THEN/ELSE IF/ELSE clauses with referenced outputs.
- **Validation:** Success if all nodes from ID12 represented and ELSE path documented; otherwise adjust.

### Conditional Logic Template
1. IF [Condition A] THEN [Action] â†’ Output: _______
2. ELSE IF [Condition B] THEN [Action] â†’ Output: _______
3. ELSE [Default Action] â†’ Output: _______

**Instruction ID 14 â€” Execution Sequence Detailing**
- **Input:** Logic blueprint and reasoning steps.
- **Process:**
  1) Align steps from reasoning and decision tree.
  2) Enumerate phases with numbered order (Initial â†’ Final).
  3) Attach expected duration or resource note for each phase (if not provided, state â€œTBDâ€).
- **Output:** Numbered execution sequence with phase name and required inputs/outputs.
- **Validation:** Success if sequence covers every reasoning step and includes at least three phases; otherwise refine.

### Execution Sequence Template
1. Phase Name â€” Inputs: ________ â€” Outputs: ________ â€” Duration/Resources: ________
2. Phase Name â€” Inputs: ________ â€” Outputs: ________ â€” Duration/Resources: ________
3. Phase Name â€” Inputs: ________ â€” Outputs: ________ â€” Duration/Resources: ________
4. ...

**Instruction ID 15 â€” Edge Case and Fallback Plan**
- **Input:** Risk analysis or historical issues.
- **Process:**
  1) Identify minimum two high-risk scenarios.
  2) For each, define detection trigger and handling action.
  3) Specify universal fallback when triggers unmet.
- **Output:** Table with columns Scenario, Trigger, Response, Owner.
- **Validation:** Success if â‰¥2 scenarios documented plus fallback row; otherwise expand.

### Edge Case Table
| Scenario | Trigger | Response | Owner |
| --- | --- | --- | --- |
| Edge Case 1 | | | |
| Edge Case 2 | | | |
| Fallback | Trigger: not met | Response: [Default handling] | Owner: |

---

## ğŸ“¤ Output: Logic Blueprint

### Generated Logic Blueprint (Ready to Copy)
```
Reasoning Reference: [From Stage 2]

Logic Pathways:
| Condition | Action | Outcome | Linked Reasoning Step |
| --- | --- | --- | --- |
| ... |

Conditional Logic:
1. IF ...
2. ELSE IF ...
3. ELSE ...

Processing Sequence:
1. Phase ...

Exception Handling:
| Scenario | Trigger | Response | Owner |
| --- | --- | --- | --- |
| ... |
```

**Instruction ID 16 â€” Transfer Stageâ€‘3 Output to Stageâ€‘4**
- **Input:** Logic Blueprint document.
- **Process:**
  1) Copy blueprint.
  2) Paste into Stageâ€‘4 input.
  3) Perform checksum comparison to ensure integrity.
- **Output:** Stageâ€‘4 input containing identical blueprint; checksum report archived.
- **Validation:** Success if checksum matches; else repeat transfer.

**Instruction ID 17 â€” Stageâ€‘4 Input Confirmation**
- **Input:** Stageâ€‘3 blueprint in Stageâ€‘4 workspace.
- **Process:**
  1) Verify presence of sections â€œLogic Pathways,â€ â€œConditional Logic,â€ â€œProcessing Sequence,â€ â€œException Handling.â€
  2) Log verification result.
- **Output:** Verification log with four checked fields.
- **Validation:** Success if all sections detected; otherwise re-import.

### Stageâ€‘4 Transfer Checklist
- Checksum Source: _______
- Checksum Destination: _______
- Match Status: [ ] Pass / [ ] Fail
- Section Verification:
  - [ ] Logic Pathways present
  - [ ] Conditional Logic present
  - [ ] Processing Sequence present
  - [ ] Exception Handling present
- Reviewer: __________________
- Timestamp: __________________

---

# Stage 4: Format Designer/Mapper (FD)

ğŸ“

**Purpose:** Define the output structure, formatting, and presentation requirements. This stage ensures your meta prompt produces results in the desired format.

---

## ğŸ“¥ Input from Previous Stage

**Instruction ID 18 â€” Format Family Selection**
- **Input:** Logic blueprint and repository of format families.
- **Process:**
  1) Score each format family against criteria (alignment with logic complexity, audience, automation requirements).
  2) Select top-scoring family.
  3) Document scorecard.
- **Output:** Statement â€œSelected Format Family: [Name] (Score X/5)â€ plus score table.
- **Validation:** Success if chosen family has highest score and justification recorded; otherwise reconsider.

### Format Family Scorecard
| Format Family | Alignment Notes | Score (1-5) |
| --- | --- | --- |
| Execution â€“ Basic Protocol | | |
| Execution â€“ Numbered Substeps | | |
| Execution â€“ Reasoning Blocks | | |
| Guidelines â€“ Rules & Standards | | |
| Issue Tracking â€“ GitHub/Jira | | |
| Prompt Engineering â€“ Multi-Role Pack | | |
| Meta-System â€“ Instruction Creator | | |
| Custom/Hybrid Format | | |

Selected Format Family: __________________ (Score __/5)

**Instruction ID 19 â€” Output Container Choice**
- **Input:** Selected format family and stakeholder delivery preferences.
- **Process:**
  1) Gather delivery constraints (e.g., platform, file type).
  2) Evaluate each optional container against constraints.
  3) Choose container meeting all mandatory constraints; if none, document exception.
- **Output:** Entry â€œSelected Output Container: [Option] â€” Constraints satisfied: [list].â€
- **Validation:** Success if selection satisfies all mandatory constraints; otherwise request alternative.

### Output Container Assessment
| Container | Mandatory Constraints Met? | Notes |
| --- | --- | --- |
| Structured Report / Narrative | | |
| Markdown Document | | |
| Table / Spreadsheet | | |
| JSON / YAML | | |
| Bullet / Checklist | | |
| CLI / Plain Text | | |
| Issue Export | | |

Selected Output Container: __________________ â€” Constraints satisfied: __________________

---

## ğŸ¨ Output Format Specification

**Instruction ID 20 â€” Output Structure Definition**
- **Input:** Format family blueprint.
- **Process:**
  1) Identify required sections from format guidelines.
  2) Populate structure template ensuring every decision node maps to at least one section component.
  3) Add numbering or identifiers for traceability.
- **Output:** Structured outline listing sections/components with IDs and descriptions.
- **Validation:** Success if every reasoning step and decision node references a component in the outline; otherwise revise.

### Output Structure Outline
1. Section ID: ________ â€” Title: ________ â€” Linked Decision Nodes: ________
   - Component 1.1: ________
   - Component 1.2: ________
2. Section ID: ________ â€” Title: ________ â€” Linked Decision Nodes: ________
   - Component 2.1: ________
   - Component 2.2: ________

**Instruction ID 21 â€” Style Guidelines Specification**
- **Input:** Audience preferences and organizational style guide.
- **Process:**
  1) Determine tone, length, complexity, POV from style guide.
  2) Document numeric or categorical values (e.g., â€œLength: 800â€“1000 wordsâ€).
  3) Note sources for each decision.
- **Output:** Table with Tone, Length (quantified), Language Complexity, Point of View, plus source column.
- **Validation:** Success if each field includes concrete value and source; else update.

### Style Guidelines Table
| Attribute | Requirement | Source |
| --- | --- | --- |
| Tone | | |
| Length | | |
| Language Complexity | | |
| Point of View | | |

**Instruction ID 22 â€” Presentation Elements Rules**
- **Input:** Publishing standards and format outline.
- **Process:**
  1) Define heading levels with examples.
  2) Specify allowed emphasis styles and frequency limits.
  3) List required tables/callouts and citation format reference.
- **Output:** Checklist detailing presentation rules with quantitative limits (e.g., â€œMax 3 callouts per sectionâ€).
- **Validation:** Success if checklist covers headings, emphasis, lists, visuals, citations with measurable constraints; otherwise extend.

### Presentation Checklist
- [ ] Heading Levels: __________________
- [ ] Emphasis Rules: __________________ (Limit: ___ per section)
- [ ] List Usage: __________________
- [ ] Visual Elements: __________________ (Max count: ___)
- [ ] Citation Format: __________________
- [ ] Verification Timestamp: __________________

---

## ğŸ“Š Data Visualization Requirements

**Instruction ID 23 â€” Visualization Requirements Definition**
- **Input:** Data visualization needs from stakeholders.
- **Process:**
  1) Determine if visuals required; record boolean.
  2) When required, specify chart type, data labels, legend position, and data source.
  3) Document rationale for each choice.
- **Output:** Visualization requirements table with fields Chart Type, Required?, Labels, Legend, Data Source, Rationale.
- **Validation:** Success if table completed for each planned visualization; otherwise mark â€œVisualization scope incomplete.â€

### Visualization Requirements Table
| Visualization | Required? (Y/N) | Chart Type | Labels | Legend Position | Data Source | Rationale |
| --- | --- | --- | --- | --- | --- | --- |
| Viz 1 | | | | | | |
| Viz 2 | | | | | | |

---

## ğŸ“ Quality Assurance

**Instruction ID 24 â€” Quality Standards Establishment**
- **Input:** Organizational QA criteria and format specification.
- **Process:**
  1) Translate Completeness, Accuracy, Clarity, Consistency into measurable checks (e.g., number of sections present, peer review performed).
  2) Define responsible reviewer.
- **Output:** QA matrix listing each criterion, metric, threshold, reviewer.
- **Validation:** Success if every criterion has quantitative threshold and assigned reviewer; else revise.

### QA Matrix
| Criterion | Metric | Threshold | Reviewer |
| --- | --- | --- | --- |
| Completeness | | | |
| Accuracy | | | |
| Clarity | | | |
| Consistency | | | |

---

## ğŸ“¤ Output: Format Specification

### Generated Format Specification (Ready to Copy)
```
Selected Format Family: [Name] (Score X/5)
Selected Output Container: [Option] â€” Constraints satisfied: [list]

Output Structure Outline:
1. Section ...

Style Guidelines:
| Attribute | Requirement | Source |
| --- | --- | --- |
| ... |

Presentation Checklist:
- Heading Levels: ...

Visualization Requirements:
| Visualization | Required? | Chart Type | Labels | Legend Position | Data Source | Rationale |
| --- | --- | --- | --- | --- | --- | --- |
| ... |

QA Matrix:
| Criterion | Metric | Threshold | Reviewer |
| --- | --- | --- | --- |
| ... |
```

**Instruction ID 25 â€” Transfer Stageâ€‘4 Output to Stageâ€‘5**
- **Input:** Format Specification document.
- **Process:**
  1) Copy Stageâ€‘4 output.
  2) Paste into Stageâ€‘5 input.
  3) Verify via checksum or diff.
- **Output:** Stageâ€‘5 input containing identical specification with verification log.
- **Validation:** Success if verification passes; otherwise repeat transfer.

**Instruction ID 26 â€” Stageâ€‘5 Input Confirmation**
- **Input:** Stageâ€‘4 content now in Stageâ€‘5 workspace.
- **Process:**
  1) Confirm presence of â€œOutput Format,â€ â€œFormatting Rules,â€ â€œPresentation Elements,â€ â€œQuality Standards.â€
  2) Mark results.
- **Output:** Confirmation checklist stored with timestamp.
- **Validation:** Success if all sections present; else re-paste.

### Stageâ€‘5 Transfer Checklist
- Verification Method: [Checksum/Diff]
- Result: [ ] Pass / [ ] Fail
- Section Confirmation:
  - [ ] Output Format present
  - [ ] Formatting Rules present
  - [ ] Presentation Elements present
  - [ ] Quality Standards present
- Reviewer: __________________
- Timestamp: __________________

---

# Stage 5: Concordance & Alignment Validator (CAV)

ğŸ“

**Purpose:** Validate consistency and alignment across all meta prompt components. This final stage ensures coherence and produces your complete, production-ready meta prompt.

---

## ğŸ” Alignment Validation

**Instruction ID 27 â€” Cross-Stage Consistency Review**
- **Input:** Completed documents from Stages 1â€“4.
- **Process:**
  1) For each checklist item, compare referenced sections (Intent vs Reasoning, etc.).
  2) Record pass/fail per item with evidence lines.
  3) Summarize findings in notes.
- **Output:** Table with four checklist items, pass/fail status, supporting citations.
- **Validation:** Success if all items marked pass or flagged with corrective action; otherwise hold progression.

### Cross-Stage Consistency Table
| Alignment Item | Pass/Fail | Evidence Reference | Corrective Action |
| --- | --- | --- | --- |
| Intent â†” Reasoning | | | |
| Reasoning â†” Logic | | | |
| Logic â†” Format | | | |
| Overall Coherence | | | |

**Instruction ID 28 â€” Component Integration Assessment**
- **Input:** Cross-stage review outputs.
- **Process:**
  1) For each alignment question, provide Yes/No/Adjust plus 1-sentence rationale referencing evidence.
  2) If â€œNeeds adjustment,â€ log corrective task.
- **Output:** Integration report summarizing alignment decisions and rationales.
- **Validation:** Success if every component has decision and rationale recorded; else revise.

### Integration Report
- Role & Intent Alignment: [Yes/No/Adjust] â€” Rationale: __________________ â€” Corrective Task ID: _______
- Reasoning & Logic Alignment: [Yes/No/Adjust] â€” Rationale: __________________ â€” Corrective Task ID: _______
- Format & Purpose Alignment: [Yes/No/Adjust] â€” Rationale: __________________ â€” Corrective Task ID: _______

**Instruction ID 29 â€” Optimization Recommendations Log**
- **Input:** Alignment gaps and inconsistencies.
- **Process:**
  1) List each gap with severity (High/Med/Low).
  2) Pair each with specific recommendation and expected impact metric.
  3) Assign owner and due date.
- **Output:** Recommendation table (Gap, Severity, Action, Impact Metric, Owner, Due Date).
- **Validation:** Success if every identified gap has action with metric and owner; otherwise update.

### Optimization Log
| Gap | Severity | Recommended Action | Impact Metric | Owner | Due Date |
| --- | --- | --- | --- | --- | --- |
| | | | | | |

---

## âœ¨ Final Meta Prompt Assembly

**Instruction ID 30 â€” Final Meta Prompt Assembly**
- **Input:** Stage outputs (1â€“4) and validation notes.
- **Process:**
  1) Merge sections into final document in prescribed order.
  2) Ensure validation status reflects latest checklist results.
  3) Run formatting lint to match style/presentation rules.
- **Output:** Final markdown document â€œMETA PROMPT â€“ FINAL VERSIONâ€ with all sections populated and lint report attached.
- **Validation:** Success if lint report passes and document contains all required sections; otherwise correct.

### Final Assembly Template
```
META PROMPT â€“ FINAL VERSION

ROLE & INTENT
[Paste from Stage 1]

REASONING FRAMEWORK
[Paste from Stage 2]

LOGIC STRUCTURE
[Paste from Stage 3]

OUTPUT FORMAT
[Paste from Stage 4]

VALIDATION STATUS
- Cross-Stage Consistency: [Pass/Fail]
- Integration Assessment: [Summary]
- QA Sign-off: [Reviewer + Timestamp]

Lint Report: [Attach results]
```

**Instruction ID 31 â€” Final Prompt Submission**
- **Input:** Production-ready meta prompt.
- **Process:**
  1) Submit document to chosen AI interface.
  2) Capture submission confirmation or response ID.
  3) Archive confirmation with timestamp.
- **Output:** Submission log including platform, time, confirmation ID.
- **Validation:** Success if confirmation ID recorded; otherwise re-submit or troubleshoot.

### Submission Log
| Platform | Submission Time | Confirmation ID | Notes |
| --- | --- | --- | --- |
| | | | |

**Instruction ID 32 â€” Post-Execution Review Log**
- **Input:** AI system outputs and stakeholder feedback.
- **Process:**
  1) Document successes with evidence snippet.
  2) Document issues with severity rating.
  3) Specify planned adjustments referencing affected sections.
- **Output:** Retrospective table (Successes, Issues, Adjustments) with evidence links.
- **Validation:** Success if each column contains at least one entry and issues include severity; otherwise follow up.

### Post-Execution Review Table
| Successes (Evidence) | Issues (Severity + Evidence) | Adjustments (Linked Section) |
| --- | --- | --- |
| | | |

**Instruction ID 33 â€” Iteration Trigger Back to Stageâ€‘1**
- **Input:** Completed review log and outstanding adjustments.
- **Process:**
  1) Evaluate whether unresolved issues exist.
  2) If yes, reopen Stageâ€‘1 template with updated requirements and note revision number.
  3) If no issues, record â€œNo iteration neededâ€ and schedule next review cycle date.
- **Output:** Iteration decision record stating whether Stageâ€‘1 restart occurs and why.
- **Validation:** Success if decision record references review evidence and, when iterating, logs new cycle start date.

### Iteration Decision Record
- Outstanding Issues? [Yes/No]
- Decision: [Restart Stage 1 / No iteration needed]
- Evidence Reference: __________________
- Next Review Cycle Date: __________________
- Authorized By: __________________

---

## âœ… Validation Summary

All instruction IDs 1â€“33 updated to match enhanced Input/Process/Output/Validation requirements. Transfer, alignment, and termination checks now include explicit verification artifacts for compliance.


---

## ğŸ“¦ Deliverables (final paste section)

### CLASSIFICATION

*(Step 1 output â€” max 2 sentences)*

### STRUCTURAL AUDIT

*(Step 2 list/table)*

### AMBIGUITY ANALYSIS

*(Step 3 tags/results)*

### COMPLETENESS VERIFICATION

*(Step 4 table)*

### ENHANCED PROTOCOL

*(Step 5 rewrite with Input / Process / Output / Validation per instruction)*

### CHANGE LOG

*(Step 6 table)*

---

## ğŸ”— Reference Snippets

**Branching Logic**

```
IF [measurable condition] THEN [action + output]
ELSE IF [measurable condition] THEN [action + output]
ELSE [default action + output]

```

**Rubric Anchors (0â€“10)**

```
0â€“2: missing/incorrect â€¢ 3â€“5: partial â€¢ 6â€“7: adequate â€¢ 8â€“9: strong â€¢ 10: exemplary (evidence cited)

```

**Validation Line**

```
Success if [metric with units/threshold]; otherwise trigger [fallback].

```